ğŸ“§ Email Spam Filter â€“ Neural Network from Scratch
A binary email spam classifier built entirely from scratch using NumPy for matrix operations â€” no machine learning libraries involved.

This project demonstrates how a simple feedforward neural network can perform surprisingly well with careful design and training, even on a small dataset.

âœ… Training Accuracy: 95.60%

âœ… Testing Accuracy: 94.00%

ğŸ§  Neural Network Architecture
The model is a fully connected feedforward neural network with:

Input Layer: 54 nodes (preprocessed features)

Hidden Layer 1: 64 nodes â€“ ReLU activation

Hidden Layer 2: 32 nodes â€“ ReLU activation

Output Layer: 1 node â€“ Sigmoid activation

ğŸ”„ Forward Propagation
Each layer computes:

ini
Copy
Edit
Z = W Â· X + b  
A = activation(Z)
Where:

W = weights

X = input from previous layer

b = bias

A = activated output

ğŸ”¢ Activation Functions
ReLU â€“ Rectified Linear Unit (Hidden Layers)
Introduces non-linearity and helps avoid vanishing gradients:

lua
Copy
Edit
ReLU(x) = max(0, x)
Sigmoid (Output Layer)
Maps final output to a probability between 0 and 1:

cpp
Copy
Edit
Sigmoid(x) = 1 / (1 + exp(-x))
ğŸ“‰ Loss Function: Binary Cross Entropy
Used to measure the difference between predicted probabilities and true binary labels:

arduino
Copy
Edit
Loss = -(1/N) * Î£ [ y * log(yÌ‚) + (1 - y) * log(1 - yÌ‚) ]
Where:

y = true label

yÌ‚ = predicted output

N = number of samples

âš™ï¸ Training & Optimization
ğŸ” Hyperparameter Tuning
A grid search was performed to tune:

Number of epochs

Learning rate

This helped identify the most effective training configuration.

ğŸ“‰ Gradient Averaging
Gradient averaging was applied over mini-batches to:

Reduce noise in weight updates

Stabilize learning

Prevent erratic parameter shifts due to small datasets

ğŸ“Š Dataset Considerations
Size: ~1000 labeled emails

Impact: A small dataset increases the risk of overfitting

Training accuracy (95.6%) slightly exceeds test accuracy (94%)

Suggests model memorization on training data

ğŸ”® Future Improvements
Several enhancements could improve performance and generalization:

Implement advanced optimizers (e.g. Adam, RMSProp)

Apply regularization techniques (e.g. L2, Dropout)

Use learning rate schedulers

Improve model architecture or depth

Collect and label more data

ğŸš€ How to Run
bash
Copy
Edit
# Clone the repository
git clone https://github.com/yourusername/email-spam-filter.git
cd email-spam-filter

# Install dependencies (only NumPy required)
pip install numpy

# Run the training script
python train.py
ğŸ“„ License
This project is licensed under the MIT License.

ğŸ¤ Contributions
Open to feedback, issues, and pull requests.

